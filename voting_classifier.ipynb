{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOC5UNsilsazOJhXqkVHbTs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/ml_works/blob/main/voting_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE8Nym-RRN5h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = pd.read_csv(\"titanic_train.csv\")\n",
        "testing_data = pd.read_csv(\"titanic_test.csv\")\n",
        "\n",
        "def get_nulls(training, testing):\n",
        "    print(\"Training Data:\")\n",
        "    print(pd.isnull(training).sum())\n",
        "    print(\"Testing Data:\")\n",
        "    print(pd.isnull(testing).sum())\n",
        "\n",
        "get_nulls(training_data, testing_data)"
      ],
      "metadata": {
        "id": "uGu35qYQRw14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different Ensemble Classification Methods\n",
        "Bagging\n",
        "ensemble_bagging\n",
        "Credit: Wikimedia Commons\n",
        "\n",
        "Bagging, also known as bootstrap aggregating, is a classification method that aims to reduce the variance of estimates by averaging multiple estimates together. Bagging creates subsets from the main dataset that the learners are trained on.\n",
        "\n",
        "In order for the predictions of the different classifiers to be aggregated, either an averaging is used for regression, or a voting approach is used for classification (based on the decision of the majority).\n",
        "\n",
        "One example of a bagging classification method is the Random Forests Classifier. In the case of the random forests classifier, all the individual trees are trained on a different sample of the dataset.\n",
        "\n",
        "The tree is also trained using random selections of features. When the results are averaged together, the overall variance decreases and the model performs better as a result.\n",
        "\n",
        "Boosting\n",
        "Boosting algorithms are capable of taking weak, underperforming models and converting them into strong models. The idea behind boosting algorithms is that you assign many weak learning models to the datasets, and then the weights for misclassified examples are tweaked during subsequent rounds of learning.\n",
        "\n",
        "The predictions of the classifiers are aggregated and then the final predictions are made through a weighted sum (in the case of regressions), or a weighted majority vote (in the case of classification).\n",
        "\n",
        "AdaBoost is one example of a boosting classifier method, as is Gradient Boosting, which was derived from the aforementioned algorithm.\n",
        "\n"
      ],
      "metadata": {
        "id": "VbHzObtNSkQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking algorithms are an ensemble learning method that combines the decision of different regression or classification algorithms. The component models are trained on the entire training dataset. After these component models are trained, a meta-model is assembled from the different models and then it's trained on the outputs of the component models. This approach typically creates a heterogeneous ensemble because the component models are usually different algorithms.\n",
        "\n"
      ],
      "metadata": {
        "id": "3ZTxqKBhSkNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "We are going to start by dropping some of the columns that will likely be useless \n",
        "- the Cabin column and the Ticket column. The Cabin column has far too many \n",
        "missing values and the Ticket column is simply comprised of too many \n",
        "categories to be useful.\n",
        "After that we will need to impute some missing values. When we do so, we must \n",
        "account for how the dataset is slightly right skewed (young ages are slightly \n",
        "more prominent than older ages). We'll use the median values when we impute the \n",
        "data because due to large outliers taking the average values would give us \n",
        "imputed values that are far from the center of the dataset:\n",
        "'''\n",
        "# Drop the cabin column, as there are too many missing values\n",
        "# Drop the ticket numbers too, as there are too many categories\n",
        "# Drop names as they won't really help predict survivors\n",
        "\n",
        "training_data.drop(labels=['Cabin', 'Ticket', 'Name'], axis=1, inplace=True)\n",
        "testing_data.drop(labels=['Cabin', 'Ticket', 'Name'], axis=1, inplace=True)\n",
        "\n",
        "# Taking the mean/average value would be impacted by the skew\n",
        "# so we should use the median value to impute missing values\n",
        "\n",
        "training_data[\"Age\"].fillna(training_data[\"Age\"].median(), inplace=True)\n",
        "testing_data[\"Age\"].fillna(testing_data[\"Age\"].median(), inplace=True)\n",
        "training_data[\"Embarked\"].fillna(\"S\", inplace=True)\n",
        "testing_data[\"Fare\"].fillna(testing_data[\"Fare\"].median(), inplace=True)\n",
        "\n",
        "get_nulls(training_data, testing_data)"
      ],
      "metadata": {
        "id": "0elN_1TjRw5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_1 = LabelEncoder()\n",
        "# Fit the encoder on the data\n",
        "encoder_1.fit(training_data[\"Sex\"])\n",
        "\n",
        "# Transform and replace training data\n",
        "training_sex_encoded = encoder_1.transform(training_data[\"Sex\"])\n",
        "training_data[\"Sex\"] = training_sex_encoded\n",
        "test_sex_encoded = encoder_1.transform(testing_data[\"Sex\"])\n",
        "testing_data[\"Sex\"] = test_sex_encoded\n",
        "\n",
        "encoder_2 = LabelEncoder()\n",
        "encoder_2.fit(training_data[\"Embarked\"])\n",
        "\n",
        "training_embarked_encoded = encoder_2.transform(training_data[\"Embarked\"])\n",
        "training_data[\"Embarked\"] = training_embarked_encoded\n",
        "testing_embarked_encoded = encoder_2.transform(testing_data[\"Embarked\"])\n",
        "testing_data[\"Embarked\"] = testing_embarked_encoded\n",
        "\n",
        "# Any value we want to reshape needs be turned into array first\n",
        "ages_train = np.array(training_data[\"Age\"]).reshape(-1, 1)\n",
        "fares_train = np.array(training_data[\"Fare\"]).reshape(-1, 1)\n",
        "ages_test = np.array(testing_data[\"Age\"]).reshape(-1, 1)\n",
        "fares_test = np.array(testing_data[\"Fare\"]).reshape(-1, 1)\n",
        "\n",
        "# Scaler takes arrays\n",
        "scaler = StandardScaler()\n",
        "\n",
        "training_data[\"Age\"] = scaler.fit_transform(ages_train)\n",
        "training_data[\"Fare\"] = scaler.fit_transform(fares_train)\n",
        "testing_data[\"Age\"] = scaler.fit_transform(ages_test)\n",
        "testing_data[\"Fare\"] = scaler.fit_transform(fares_test)"
      ],
      "metadata": {
        "id": "ccCDzJVoRw7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now to select our training/testing data\n",
        "X_features = training_data.drop(labels=['PassengerId', 'Survived'], axis=1)\n",
        "y_labels = training_data['Survived']\n",
        "\n",
        "print(X_features.head(5))\n",
        "\n",
        "# Make the train/test data from validation\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_features, \n",
        "                                                  y_labels, \n",
        "                                                  test_size=0.1, \n",
        "                                                  random_state=27)"
      ],
      "metadata": {
        "id": "BlE0o3m_TTyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple average approach\n",
        "'''\n",
        "Classification is done by individual classification models and then we average \n",
        "it out.\n",
        "'''\n",
        "LogReg_clf = LogisticRegression()\n",
        "DTree_clf = DecisionTreeClassifier()\n",
        "SVC_clf = SVC()\n",
        "\n",
        "LogReg_clf.fit(X_train, y_train)\n",
        "DTree_clf.fit(X_train, y_train)\n",
        "SVC_clf.fit(X_train, y_train)\n",
        "\n",
        "LogReg_pred = LogReg_clf.predict(X_val)\n",
        "DTree_pred = DTree_clf.predict(X_val)\n",
        "SVC_pred = SVC_clf.predict(X_val)\n",
        "\n",
        "averaged_preds = (LogReg_pred + DTree_pred + SVC_pred)//3\n",
        "acc = accuracy_score(y_val, averaged_preds)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "wo4_KJiURw-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Voting Classifier\n",
        "'''\n",
        "voting classifier is trained.\n",
        "'''\n",
        "voting_clf = VotingClassifier(estimators=[('SVC', SVC_clf), \n",
        "                                          ('DTree', DTree_clf), \n",
        "                                          ('LogReg', LogReg_clf)], \n",
        "                              voting='hard')\n",
        "voting_clf.fit(X_train, y_train)\n",
        "preds = voting_clf.predict(X_val)\n",
        "acc = accuracy_score(y_val, preds)\n",
        "l_loss = log_loss(y_val, preds)\n",
        "f1 = f1_score(y_val, preds)\n",
        "\n",
        "print(\"Accuracy is: \" + str(acc))\n",
        "print(\"Log Loss is: \" + str(l_loss))\n",
        "print(\"F1 Score is: \" + str(f1))"
      ],
      "metadata": {
        "id": "UJr1_AR4RxBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bagging classification\n",
        "logreg_bagging_model = BaggingClassifier(base_estimator=LogReg_clf, \n",
        "                                         n_estimators=50, \n",
        "                                         random_state=12)\n",
        "dtree_bagging_model = BaggingClassifier(base_estimator=DTree_clf, \n",
        "                                        n_estimators=50, \n",
        "                                        random_state=12)\n",
        "random_forest = RandomForestClassifier(n_estimators=100, \n",
        "                                       random_state=12)\n",
        "extra_trees = ExtraTreesClassifier(n_estimators=100, \n",
        "                                   random_state=12)\n",
        "\n",
        "def bagging_ensemble(model):\n",
        "    k_folds = KFold(n_splits=20, random_state=12, shuffle = True)\n",
        "    results = cross_val_score(model, X_train, y_train, cv=k_folds)\n",
        "    print(results.mean())\n",
        "\n",
        "bagging_ensemble(logreg_bagging_model)\n",
        "bagging_ensemble(dtree_bagging_model)\n",
        "bagging_ensemble(random_forest)\n",
        "bagging_ensemble(extra_trees)"
      ],
      "metadata": {
        "id": "-YHT9Og6RxEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bagging with kfold\n",
        "k_folds = KFold(n_splits=20, random_state=12, shuffle = True)\n",
        "\n",
        "num_estimators = [20, 40, 60, 80, 100]\n",
        "\n",
        "for i in num_estimators:\n",
        "    ada_boost = AdaBoostClassifier(n_estimators=i, random_state=12)\n",
        "    results = cross_val_score(ada_boost, X_train, y_train, cv=k_folds)\n",
        "    print(\"Results for {} estimators:\".format(i))\n",
        "    print(results.mean())"
      ],
      "metadata": {
        "id": "OqWAsrq5RxHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B2YW_PKqRxKS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}