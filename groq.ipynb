{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdJi3avH48COq13NawykVD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/ml_works/blob/main/groq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omalcr7saE56",
        "outputId": "1fd9cda2-b17a-4044-a137-51db6beb842a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import groq\n",
        "from groq import Groq\n",
        "import linecache\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "38znl7fsaHkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GROQ_API_KEY'] = linecache.getline('groq.txt', 1).strip()"
      ],
      "metadata": {
        "id": "74G08vfAaHnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Groq(api_key=linecache.getline('groq.txt', 1).strip())\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Explain the importance of temperature in large language models\"}\n",
        "    ],\n",
        "    model=\"llama3-70b-8192\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-ee1SRJaHrS",
        "outputId": "d4527ccb-2b9a-433f-e528-8af1cb515393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature is a crucial hyperparameter in large language models, particularly in those using softmax output layers, such as transformer-based models like BERT, RoBERTa, and XLNet. It plays a significant role in controlling the model's output distribution, influencing the generated text's diversity, fluency, and coherence.\n",
            "\n",
            "**What is temperature in language models?**\n",
            "\n",
            "In the context of language models, temperature (τ) is a scalar value that adjusts the output probabilities of the softmax layer. The softmax function maps the input logits (unnormalized scores) to a probability distribution over the vocabulary. The temperature parameter scales the logits before applying the softmax function:\n",
            "\n",
            "`output_probabilities = softmax(logits / τ)`\n",
            "\n",
            "**Effects of temperature on language model output:**\n",
            "\n",
            "1. **Diversity**: Lower temperatures (τ → 0) result in more peaked output distributions, favoring the most likely tokens and reducing diversity in the generated text. Higher temperatures (τ → ∞) lead to more uniform distributions, increasing the likelihood of less common tokens and promoting diversity.\n",
            "2. **Fluency**: Lower temperatures can lead to more fluent text, as the model is more likely to generate common, well-formed sentences. Higher temperatures may result in less fluent text, as the model is more likely to produce unusual or grammatically incorrect sentences.\n",
            "3. **Coherence**: Lower temperatures can improve coherence by favoring tokens that are more likely to form coherent sentences. Higher temperatures may lead to less coherent text, as the model is more likely to generate tokens that are less related to the context.\n",
            "4. **Exploration-exploitation trade-off**: Temperature controls the exploration-exploitation trade-off in language models. Lower temperatures encourage exploitation of the most likely tokens, while higher temperatures promote exploration of less common tokens.\n",
            "\n",
            "**Importance of temperature in large language models:**\n",
            "\n",
            "1. **Controlling output diversity**: Temperature allows for adjusting the level of diversity in the generated text, which is essential in applications like text generation, language translation, and chatbots.\n",
            "2. **Improving fluency and coherence**: By tuning the temperature, models can be encouraged to generate more fluent and coherent text, which is critical in applications like language translation, text summarization, and dialogue systems.\n",
            "3. **Enhancing creativity**: Higher temperatures can lead to more creative and diverse text, making them suitable for applications like language generation, poetry, and storytelling.\n",
            "4. **Balancing exploration and exploitation**: Temperature helps strike a balance between exploring new possibilities and exploiting the most likely tokens, which is essential in applications like language modeling, machine translation, and text classification.\n",
            "\n",
            "**Best practices for temperature tuning:**\n",
            "\n",
            "1. **Start with a low temperature**: Begin with a low temperature (e.g., 0.1) and gradually increase it to promote diversity and exploration.\n",
            "2. **Monitor output quality**: Evaluate the generated text's quality, fluency, and coherence, and adjust the temperature accordingly.\n",
            "3. **Use temperature scheduling**: Implement temperature scheduling, where the temperature is adjusted during training or inference to promote exploration or exploitation at different stages.\n",
            "4. **Experiment with different temperatures**: Try different temperatures for different tasks or applications to find the optimal setting.\n",
            "\n",
            "In summary, temperature is a crucial hyperparameter in large language models, as it controls the output distribution, diversity, fluency, and coherence of the generated text. By tuning the temperature, models can be optimized for specific tasks, and the exploration-exploitation trade-off can be balanced to achieve better performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vh9naCtlaHu1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}