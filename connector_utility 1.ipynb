{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "81007ae4-2ac3-495b-b97b-6a28c5c9030c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">/databricks/python/lib/python3.8/site-packages/snowflake/connector/options.py:96: UserWarning: You have an incompatible version of &#39;pyarrow&#39; installed (7.0.0), please install a version that adheres to: &#39;pyarrow&lt;6.1.0,&gt;=6.0.0; extra == &#34;pandas&#34;&#39;\n",
       "  warn_incompatible_dep(\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">/databricks/python/lib/python3.8/site-packages/snowflake/connector/options.py:96: UserWarning: You have an incompatible version of &#39;pyarrow&#39; installed (7.0.0), please install a version that adheres to: &#39;pyarrow&lt;6.1.0,&gt;=6.0.0; extra == &#34;pandas&#34;&#39;\n  warn_incompatible_dep(\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#required packages\n",
    "\n",
    "import pandas as pd, numpy as np, databricks.koalas as ks\n",
    "from datetime import datetime, date, timedelta, tzinfo\n",
    "import pytz\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from snowflake.sqlalchemy import URL\n",
    "import sqlalchemy as sal\n",
    "from sqlalchemy import create_engine\n",
    "import snowflake.connector\n",
    "%matplotlib inline\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "from cryptography.hazmat.primitives import serialization, hashes, hmac\n",
    "import re\n",
    "from databricks import sql\n",
    "import logging as logger\n",
    "from retrying import retry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c92f9a7c-70c7-4676-b41c-5d6db5274d64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#declare variables\n",
    "\n",
    "#snowflake\n",
    "user_nm='SPASU05'\n",
    "scope_nm='SNF-DOPS-USER-DB-{}-SCP'.format(user_nm)\n",
    "\n",
    "\n",
    "sf_connection_dict={\n",
    "                 'EDW_environment' : 'edw',\n",
    "                 'EDW_regular_url' : 'abs_edw_prd.west-us-2.privatelink.snowflakecomputing.com',\n",
    "                 'EDW_regular_database' : 'DW_PRD',\n",
    "                 'EDW_regular_schema' : 'TEMP_DS',\n",
    "                 'EDW_regular_warehouse' : 'PROD_DATA_ANALYTICS_WH',\n",
    "                 'EDW_regular_role' : 'EDDM_DATA_SCIENTIST_GG',\n",
    "                 'EDM_environment' : 'edm',\n",
    "                 'EDM_regular_url' : 'abs_itds_prd.west-us-2.privatelink.snowflakecomputing.com',\n",
    "                 'EDM_regular_database' : 'EDM_FEATURES_PRD',\n",
    "                 'EDM_regular_schema' : 'SCRATCH_DS',\n",
    "                 'EDM_regular_warehouse' : 'EDM_DATASCIENCE_WH',\n",
    "                 'EDM_regular_role' : 'EDDM_DATA_SCIENTIST_GG'                         \n",
    "                }\n",
    "\n",
    "snfKey  = dbutils.secrets.get(scope=scope_nm, key=\"SnowEncPswdKey\")\n",
    "snfPass = dbutils.secrets.get(scope=scope_nm, key=\"SnowEncPswdPass\")\n",
    "snfUser = dbutils.secrets.get(scope=scope_nm, key=\"SnowUsername\")\n",
    "\n",
    "\n",
    "#ADLS\n",
    "adls_connection_dict={'adls_server_hostname': 'adb-3055846038102621.1.azuredatabricks.net',\n",
    "                      'http_path': 'sql/protocolv1/o/3055846038102621/0315-150338-eerie83',\n",
    "                      'access_token_scope':'scp_itds_devops',\n",
    "                      'access_token_key' : 'dasc-prod-databricks-01-dbx-key'                        \n",
    "                          }\n",
    "#db_work dev/prod paths\n",
    "pathADLSdev = 'abfs://absitdsdevwussa001@absitdsdevwussa001.dfs.core.windows.net/itds/dev/default/work/'\n",
    "pathADLSprd = 'abfs://absitdsprodwussa001@absitdsprodwussa001.dfs.core.windows.net/default/work/datascience/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "92304580-e840-46c9-850c-a7b2303e6718",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Function to get private key\n",
    "def getKey(passwordKey, passPhrase):\n",
    "  p_key = serialization.load_pem_private_key(\n",
    "      passwordKey.encode(),\n",
    "      password=passPhrase.encode(),\n",
    "      backend=default_backend()\n",
    "  )\n",
    "\n",
    "  pkb = p_key.private_bytes(\n",
    "         encoding=serialization.Encoding.PEM,\n",
    "         format=serialization.PrivateFormat.PKCS8,\n",
    "         encryption_algorithm=serialization.NoEncryption()\n",
    "        )\n",
    "\n",
    "  pkb = pkb.decode(\"UTF-8\")\n",
    "  return re.sub(\"-*(BEGIN|END) PRIVATE KEY-*\\n\",\"\",pkb).replace(\"\\n\",\"\")\n",
    "\n",
    "'''snowflake functions '''\n",
    "\n",
    "\n",
    "'''function to create snowflake table''' \n",
    "def create_table_snowflake(df, snowflake_table):\n",
    "  \n",
    "    snow_engine = snowflake_connector()\n",
    "    index = df.index\n",
    "    for i in range(len(df)//15000 + 1):\n",
    "        df.iloc[(i)*15000:(i+1)* 15000].to_sql(snowflake_table, con=snow_engine, if_exists='replace', index=False, index_label=None)\n",
    "\n",
    "        \n",
    "'''function to read snowflake data'''\n",
    "@retry(stop_max_attempt_number=2)\n",
    "def read_snowflake(env,sql, role='regular', sys='prd'):\n",
    "  try:\n",
    "    if (env == 'edw' and role == 'regular'):\n",
    "      swOptions = dict(sfUrl=sf_connection_dict['EDW_regular_url'],\n",
    "               sfUser=snfUser,\n",
    "               pem_private_key=getKey(snfKey, snfPass),\n",
    "               sfDatabase=sf_connection_dict['EDW_regular_database'],\n",
    "               sfSchema=sf_connection_dict['EDW_regular_schema'],\n",
    "               sfWarehouse=sf_connection_dict['EDW_regular_warehouse'],\n",
    "               sfRole=sf_connection_dict['EDW_regular_role'])\n",
    "    elif (env == 'edm' and role == 'regular'):\n",
    "      swOptions = dict(sfUrl=sf_connection_dict['EDM_regular_url'],\n",
    "               sfUser=snfUser,\n",
    "               pem_private_key=getKey(snfKey, snfPass),\n",
    "               sfDatabase=sf_connection_dict['EDM_regular_database'],\n",
    "               sfSchema=sf_connection_dict['EDM_regular_schema'],\n",
    "               sfWarehouse=sf_connection_dict['EDM_regular_warehouse'],\n",
    "               sfRole=sf_connection_dict['EDM_regular_role'])\n",
    "    else:\n",
    "      raise Exception('Configuration not found for environment {}.'.format(env))\n",
    "  except Exception as e:\n",
    "    raise Exception('Configuration not found for environment {}.'.format(env, e))\n",
    "  df = spark.read\\\n",
    "    .format(\"snowflake\")\\\n",
    "    .options(**swOptions)\\\n",
    "    .option(\"query\", sql)\\\n",
    "    .load()\n",
    "  df = df.toDF(*[c.lower() for c in df.columns])\n",
    "  return df\n",
    "\n",
    "'''function to write data to snowflake table'''\n",
    "def write_snowflake(env, sql, table, write_mode, role='regular', sys='prd'):\n",
    "  try:\n",
    "      if (env == 'edw' and role == 'regular'):\n",
    "        swOptions = dict(sfUrl=sf_connection_dict['EDW_regular_url'],\n",
    "               sfUser=snfUser,\n",
    "               pem_private_key=getKey(snfKey, snfPass),\n",
    "               sfDatabase=sf_connection_dict['EDW_regular_database'],\n",
    "               sfSchema=sf_connection_dict['EDW_regular_schema'],\n",
    "               sfWarehouse=sf_connection_dict['EDW_regular_warehouse'],\n",
    "               sfRole=sf_connection_dict['EDW_regular_role'])\n",
    "      elif (env == 'edm' and role == 'regular'):\n",
    "        swOptions = dict(sfUrl=sf_connection_dict['EDM_regular_url'],\n",
    "               sfUser=snfUser,\n",
    "               pem_private_key=getKey(snfKey, snfPass),\n",
    "               sfDatabase=sf_connection_dict['EDM_regular_database'],\n",
    "               sfSchema=sf_connection_dict['EDM_regular_schema'],\n",
    "               sfWarehouse=sf_connection_dict['EDM_regular_warehouse'],\n",
    "               sfRole=sf_connection_dict['EDM_regular_role'])\n",
    "      else:\n",
    "        raise Exception('Configuration not found for environment {}.'.format(env))\n",
    "  except Exception as e:\n",
    "    raise Exception('Configuration not found for environment {}.'.format(env, e))\n",
    "\n",
    "  (spark.sql(sql)\n",
    "     .write\n",
    "     .mode(write_mode)\n",
    "     .format(\"snowflake\")\n",
    "     .options(**swOptions)\n",
    "     .option(\"truncate_table\", \"on\")\n",
    "     .option(\"usestagingtable\", \"off\")\n",
    "     .option(\"dbtable\", table)\n",
    "     .save()\n",
    "  ) \n",
    "  \n",
    "  return\n",
    "\n",
    "\n",
    "'''function to check if snowflake table is empty''' \n",
    "\n",
    "def empty_snowflake_table_check(env, table):\n",
    "  record_existence = 1\n",
    "  try:\n",
    "    sql = \"\"\"SELECT * FROM {} limit 100 \"\"\".format(table)\n",
    "    snf_record_existence =  read_snowflake(env,sf_connection_dict, sql)\n",
    "    snf_record_existence.createOrReplaceTempView(\"snf_record_existence\")\n",
    "    record_existence =   spark.sql(\"\"\"SELECT count(*) as cnt FROM snf_record_existence \"\"\" ).collect()[0][0]\n",
    "    print(record_existence)\n",
    "    if record_existence == 0:\n",
    "      return \"\"\"Empty Table Pre-requsite met for {}\"\"\".format(table)\n",
    "    else:\n",
    "      raise Exception(\"\"\"Empty Table Pre-requsite not met for {}. Record not found.\"\"\".format(table))\n",
    "  except Exception as e:\n",
    "    raise Exception('Empty Table Pre-requsite not met for {}.'.format(table))\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "''' \n",
    "generic functions\n",
    "convert spark df to koalas\n",
    "'''\n",
    "\n",
    "def read_sw_k(env, sql, role='regular'):\n",
    "  df = read_snowflake(env, sql, role)\n",
    "  df = df.to_koalas()\n",
    "  return df\n",
    "\n",
    "# convert spark df to pandas\n",
    "def read_sw_p(env, sql, role='regular'):\n",
    "  df = read_snowflake(env, sql, role)\n",
    "  df = df.toPandas()\n",
    "  return df\n",
    "\n",
    "def list_to_string(x):\n",
    "  return ', '.join(map(str, sorted(x)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''ADLS fucntions to access Hive tables stored as parquet '''   \n",
    "\n",
    "\n",
    "'''create ADLS connector '''\n",
    "def get_connector(adls_connection_dict):\n",
    "    print(adls_connection_dict['adls_server_hostname'])\n",
    "    connection = sql.connect(\n",
    "        server_hostname=adls_connection_dict['adls_server_hostname'],\n",
    "        http_path=adls_connection_dict['http_path'],\n",
    "        access_token = dbutils.secrets.get(scope=adls_connection_dict['access_token_scope'], key=adls_connection_dict['access_token_key']))\n",
    "    return connection\n",
    "  \n",
    "'''function to check record existence in hive(db_work) table '''  \n",
    "def record_existence_check(table):\n",
    "  record_existence = 0\n",
    "  try:\n",
    "    record_existence =   spark.sql(\"\"\"SELECT count(*) as cnt\n",
    "                            FROM {}\n",
    "                            \"\"\".format(table )\n",
    "                            ).collect()[0][0]\n",
    "    if record_existence > 0:\n",
    "      return \"\"\"Pre-requsite met for {}\"\"\".format(table)\n",
    "    else:\n",
    "      raise Exception(\"\"\"Pre-requsite not met for {}. Record not found.\"\"\".format(table))\n",
    "  except Exception as e:\n",
    "    raise Exception('Pre-requsite not met for {}. Error: {}'.format(table, e))\n",
    "    \n",
    "    \n",
    "'''\n",
    "function to check if table was prperly saved in ADLS\n",
    "'''\n",
    "def update_adls_table_check(table):\n",
    "  try:\n",
    "    q =   spark.sql(\"\"\"DESCRIBE DETAIL {}\n",
    "                            \"\"\".format(table)\n",
    "                            ).collect()[0][5]\n",
    "    check_dt = datetime.strftime(q, '%Y-%m-%d')\n",
    "    if check_dt == Prediction_date:\n",
    "      return \"\"\"Pre-requsite met for {}\"\"\".format(table)\n",
    "    else:\n",
    "      raise Exception(\"\"\"Pre-requsite not met for {}. Table is not properly saved/replaced in adls gen 2 .\"\"\".format(table))\n",
    "  except Exception as e:\n",
    "    raise Exception('Pre-requsite not met for {}.'.format(table))\n",
    "\n",
    "'''\n",
    "function to write/append data to db_work tables\n",
    "'''\n",
    "    \n",
    "def write_append_parquet(path,name,df,write_mode):  \n",
    "  df.write.format(\"parquet\").mode(write_mode).option(\"path\", f'{path}{name}').saveAsTable(f\"db_work.{name.replace('/','_')}\")\n",
    "    \n",
    "'''\n",
    "function to drop hive table\n",
    "'''\n",
    "\n",
    "def drop_hive_table(table):\n",
    "    query = '''DROP TABLE IF EXISTS {}'''.format(table)\n",
    "    return spark.sql(query)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "639f14e7-b6a7-43ef-b49d-cee19f300747",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# q = \"\"\"select distinct w.division_id, w.warehouse_id, w.warehouse_nm,\n",
    "# rog.rog_id, a.corp_item_cd as corporate_item_cd, desc_item as item_dsc, ctgry_cd as smic_category, pack_whse as ship_unit_pack_qty\n",
    "# from scratch_ds.ia_cic_master a\n",
    "# join edm_views_prd.dw_views.corporate_item c on (a.corp_item_cd = c.corporate_item_cd)\n",
    "# join edm_views_prd.dw_views.supply_chain_item b\n",
    "# on (c.corporate_item_integration_id = b.corporate_item_integration_id)\n",
    "# join edm_views_prd.dw_views.warehouse w\n",
    "# on (b.warehouse_id = w.warehouse_id and b.division_id = w.division_id)\n",
    "# join edm_views_prd.dw_views.retail_order_group rog\n",
    "# on (rog.division_id = w.division_id)\n",
    "# where c.DW_CURRENT_VERSION_IND = TRUE\n",
    "# and c.DW_LOGICAL_DELETE_IND = FALSE\n",
    "# and b.DW_CURRENT_VERSION_IND = TRUE\n",
    "# and b.DW_LOGICAL_DELETE_IND = FALSE\n",
    "# and b.division_id = 32\n",
    "# and w.distribution_center_id = 'WMEL'\n",
    "# and w.DW_CURRENT_VERSION_IND = TRUE\n",
    "# and w.DW_LOGICAL_DELETE_IND = FALSE\n",
    "# and rog.DW_CURRENT_VERSION_IND = TRUE\n",
    "# and rog.DW_LOGICAL_DELETE_IND = FALSE\"\"\"\n",
    "\n",
    "# read_snowflake('edm', q, 'regular' ,'prd').count()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "connector_utility",
   "notebookOrigID": 963814889367431,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
