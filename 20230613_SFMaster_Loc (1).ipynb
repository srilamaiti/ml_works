{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import requests\n",
    "import snowflake\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use your own credentials\n",
    "username='username'\n",
    "useremail='email'\n",
    "password='SBDpassword'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the code to link SnowFlake Database, do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oauth_token(svc_username=None, svc_password=None):\n",
    "    \"\"\"\n",
    "    Retrives the authentication token for SBD Snowflake.\n",
    "    This function automatically requests for username and password from the user through interactive prompts.\n",
    "    If using service account credentials, they can be passed as svc_username and svc_password.\n",
    "    Keywords:\n",
    "    svc_username -- Service account username for which the token should be provided.\n",
    "    svc_password -- Password corresponding to the service account user.\n",
    "    Example usage:\n",
    "    # Authentication with your personal user id.\n",
    "    from sbd_common_utils.snowflake_utils import get_oauth_token\n",
    "    access_token = get_oauth_token()\n",
    "    # Authentication with service account.\n",
    "    from sbd_common_utils.snowflake_utils import get_oauth_token\n",
    "    from sbd_common_utils.common_utils import get_service_account_creds\n",
    "    username, password = get_service_account_creds(\"/datascience/sandbox/someapp/service-account\")\n",
    "    access_token = get_oauth_token(username, password)\n",
    "    \"\"\"\n",
    "\n",
    "    if svc_username and svc_password:\n",
    "        username = svc_username\n",
    "        password = svc_password\n",
    "    else:\n",
    "        print(\"Username and password isn't provided. Requesting user...\")\n",
    "        username = os.environ[\"service_account_id\"]\n",
    "        password = get_password_util(\n",
    "                pass_value=os.environ.get(\"service_account_password\", None),\n",
    "                pass_name=\"Please Enter Service Account Password:  \",\n",
    "            )\n",
    "\n",
    "\n",
    "    r = requests.post(\n",
    "        \"https://ssoprod.sbdinc.com/as/token.oauth2\",\n",
    "        data={\n",
    "            \"client_id\": \"Snowflake\",\n",
    "            \"grant_type\": \"password\",\n",
    "            \"username\": username,\n",
    "            \"password\": password,\n",
    "            \"client_secret\": 'f9sq630wmLP6UjpSsOk7kTuP6xccCrSOC4YhE1VdTq3GCupqR7gjYcpuhEGRJ9e0',\n",
    "            \"scope\": \"session:role-any\",\n",
    "        },\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    access_token = r.json()[\"access_token\"]\n",
    "    return access_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsnowflakecursor(snowflakedatabase,\n",
    "                       snowflakewarehouse,\n",
    "                       snowflakeschema,\n",
    "                       snowflakerole,\n",
    "                       username,\n",
    "                       userpassword\n",
    "                      ):\n",
    "    import requests\n",
    "    import snowflake.connector\n",
    "    \n",
    "    \n",
    "\n",
    "    access_token = get_oauth_token(svc_username=username,\n",
    "                               svc_password=userpassword)\n",
    "\n",
    "    # Gets the version\n",
    "    activesnowflakeconnector = snowflake.connector.connect(account='sbd_caspian.us-east-1', \n",
    "                                                           authenticator='oauth', \n",
    "                                                           token=access_token,\n",
    "                                                          warehouse=snowflakewarehouse,\n",
    "                                                          database=snowflakedatabase,\n",
    "                                                          role=snowflakerole,\n",
    "                                                          schema=snowflakeschema)\n",
    "    activesnowflakecursor = activesnowflakeconnector.cursor()\n",
    "    \n",
    "\n",
    "\n",
    "    sqlalchemyengine = create_engine(f\"snowflake://sbd_caspian.us-east-1.snowflakecomputing.com\", creator=lambda: activesnowflakeconnector)\n",
    "    \n",
    "\n",
    "    return(activesnowflakeconnector,activesnowflakecursor,sqlalchemyengine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_configs(snowflakedatabase,\n",
    "                       snowflakewarehouse,\n",
    "                       snowflakeschema,\n",
    "                       snowflakerole,\n",
    "                       username,\n",
    "                       userpassword,\n",
    "                     useremail):\n",
    "    \n",
    "    \n",
    "    spark_snowflake_configs = dict(\n",
    "                sfUrl = 'sbd_caspian.us-east-1.snowflakecomputing.com',\n",
    "                sfUser = useremail,  \n",
    "                sfAuthenticator = \"oauth\",\n",
    "                sfRole = snowflakerole,\n",
    "                sfDatabase = snowflakedatabase,\n",
    "                sfSchema = snowflakeschema,\n",
    "                sfWarehouse = snowflakewarehouse,  \n",
    "                sfToken=get_oauth_token(username,userpassword)\n",
    "                )\n",
    "    return(spark_snowflake_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_oauth_token(username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can modify the cell below (dabase, role, schemas) before reading tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activesnowflakeconnector,activesnowflakecursor,sqlalchemyengine = getsnowflakecursor(snowflakedatabase='PROD_EDW',\n",
    "                                                                                     snowflakewarehouse='DEV_AIDA_WH',\n",
    "                                                                                     snowflakeschema = 'DIMENSIONS',\n",
    "                                                                                     snowflakerole = 'OPERATIONS_CREATOR_RO',\n",
    "                                                                                     username = username,\n",
    "                                                                                     userpassword = password\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below are the codes to generate location master file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querytorun = 'SELECT * FROM PROD_EDW.DIMENSIONS.DIM_LOCATION'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_sql(querytorun,sqlalchemyengine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column with all Null values\n",
    "df1a = df1.dropna(axis =1, how = 'all')\n",
    "df1a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select the useful information for a location\n",
    "col1= ['loc_key', 'src_sys_key', 'loc_id','src_rcrd_create_dte', 'src_rcrd_upd_dte','eff_dte', 'loc_addr_key', 'cntct_phn_nbr',\n",
    "       'loc_typ_cd','loc_name', 'loc_desc','loc_regn_cd', 'systen_plnr_name' ]\n",
    "df1b = df1[col1].sort_values(by = ['loc_id', 'loc_key'], ascending= [True, True]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b['src_sys_key'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b['plant_no'] = df1b['loc_key'].str.extract('~(.*?)~').astype(str)\n",
    "df1b['plant_no2'] = df1b['loc_key'].str.split('~').str[1]\n",
    "\n",
    "# Some plant has to loc_id, extracted from the loc_key\n",
    "df1b.loc[df1b['plant_no'] == 'nan', 'plant_no'] = df1b['plant_no2']\n",
    "df1b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortSAP = ['SAPE03', 'SAPC11', 'SAPP10','SAPSHP', 'QADCH', 'QADAR', 'QADBR', 'QADPE','JDAEDW', 'LEGACYWMS', 'LAWSONMAC']\n",
    "df1b['src_sys_key'] = pd.Categorical(df1b['src_sys_key'], categories= sortSAP, ordered= True )\n",
    "df1c = df1b.sort_values(by = ['src_sys_key', 'plant_no'], ascending= [True, True]).drop_duplicates('plant_no').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectSAP = ['SAPE03', 'SAPC11', 'SAPP10','SAPSHP', 'QADCH', 'QADAR', 'QADBR', 'QADPE']\n",
    "df1d = df1c[df1c['src_sys_key'].isin(selectSAP) == True].reset_index(drop = True)\n",
    "\n",
    "df1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addess = df1d['loc_addr_key'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read fromt the Address Table and merge to the location table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reason to link Add_nbr in the sql query because address table is too large to read into a dataframe efficiently\n",
    "queryaddress = \"\"\"\n",
    "select a.SRC_SYS_KEY\n",
    ", a.EFF_DTE\n",
    ", a.CITY_NAME\n",
    ", a.REGN_LKEY\n",
    ", a.CITY_PSTL_CD\n",
    ", a.CNTRY_KEY\n",
    ", a.CNTRY_DESC\n",
    ", a.ADDR_1\n",
    ", a.ADDR_GRP_LKEY\n",
    ", a.ADDR_NBR\n",
    ", a.MATCH_CD_NAME\n",
    "from prod_edw.dimensions.dim_address a inner join prod_edw.dimensions.DIM_LOCATION l on a.ADDR_NBR = l.loc_addr_key\n",
    "\"\"\"\n",
    "\n",
    "df2 = pd.read_sql(queryaddress,sqlalchemyengine)\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df1d, df2, how = 'left', left_on = 'loc_addr_key', right_on = 'addr_nbr', suffixes=('', '_drop')).reset_index(drop = True)\n",
    "df3.drop([col for col in df3.columns if 'drop' in col], axis=1, inplace=True)\n",
    "\n",
    "df3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3a = df3[['loc_key', 'src_sys_key', 'loc_id', 'plant_no',\n",
    "       'src_rcrd_upd_dte', 'eff_dte', 'loc_addr_key', 'loc_name','match_cd_name',\n",
    "        'city_name', 'regn_lkey', 'city_pstl_cd', 'cntry_key', 'cntry_desc',]].reset_index(drop = True)\n",
    "\n",
    "#remove duplicates\n",
    "df3b = df3a.sort_values(by = ['loc_key', 'plant_no', 'loc_name', 'match_cd_name'], ascending = [True, True, True, True]).drop_duplicates(['loc_key', 'plant_no'])\n",
    "\n",
    "df3b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file has been saved in to the shared folder, no need to generate every time, the path should be modified based on the actual user's path\n",
    "pathloc = 'C:\\\\Users\\\\username\\\\Stanley Black & Decker\\\\Supply Chain Development - General\\\\Projects\\\\Data Collected\\\\SF data and sample code\\\\'\n",
    "df3b.to_csv(pathloc+'20230613_SFMaster_Location.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
