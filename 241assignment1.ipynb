{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/ml_works/blob/main/241assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sou8pYm5suJU"
      },
      "source": [
        "# Assignment 1: Projective Transform\n",
        "\n",
        "MIDS W281: Computer Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q-A1P_OsuJX"
      },
      "source": [
        "## Recommended Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f-_XIubsuJX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import sin, cos\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import imageio\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy43NPxJsuJY"
      },
      "source": [
        "## Part 1: Pole Height"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W6uEImjsuJY"
      },
      "source": [
        "![Hybrid Teaser](https://raw.githubusercontent.com/W281/fileRepository/main/Assignments/Assignment_1/pole_length.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt3-QHyHsuJY"
      },
      "source": [
        "### Overview\n",
        "Single-view metrology corresponds to a set of techniques for making 3-D measurements from a single 2-D image. Such measurements have several applications including, for example, measuring the height of a suspect in a crime-scene image. We will consider a simple scenario of single-view metrology estimating 3-D measurements of objects in a rendered 2-D image.\n",
        "\n",
        "### Description\n",
        "\n",
        "Consider the image shown in Figure 1. You are given the following information about the 3-D scene. There are three poles colored yellow, green, and blue, and a ground plane in this scene. The three poles are perpendicular to the ground plane. The ground plane is parallel to the X-Z plane, and the camera's optical axis is parallel to the Z-axis. Standing at 4 meters, the yellow and green poles are of the same height in the 3-D scene. The distance between these two poles is 17 meters. The green and blue poles are at the same, but unknown, distance from the camera. Finally, the resolution of the image sensor of the camera is 53,333 pixels per meter.\n",
        "\n",
        "Recall the perspective projection equation that tells us the relationship between a point's location in the 3-D world (X,Y,Z), the camera focal length (f), and the projection of that point in the image (x,y):  \n",
        "$$\\frac{x}{X}=\\frac{f}{Z}$$\n",
        "and\n",
        "$$\\frac{y}{Y}=\\frac{f}{Z}$$\n",
        "&nbsp;\n",
        "\n",
        "Write a python script to measure the heights and distances between the poles in image coordinates. You may use the pixel locations provided below to estimate the location of the pole endpoints in the image:\n",
        "\n",
        "$$yellow = [[518, 391],  [512, 987]]$$\n",
        "$$green = [[733, 462],  [732, 771]]$$\n",
        "$$blue = [[1189, 311], [1189, 769]]$$\n",
        "\n",
        "From these image measurements and the 3-D quantities specified above, you will be able to determine the camera focal length and 3-D height of the blue pole."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_density_per_meter = 53333\n",
        "Y_ylw = 4\n",
        "Y_grn = 4\n",
        "\n",
        "# Yellow pole, image coordinate system\n",
        "x_ylw_x1, x_ylw_x2 = 518, 512\n",
        "y_ylw_y1, y_ylw_y2 = 391, 987\n",
        "\n",
        "# Green pole, image coordinate system\n",
        "x_grn_x1, x_grn_x2 = 733, 732\n",
        "y_grn_y1, y_grn_y2 = 462, 771\n",
        "\n",
        "# Blue pole, image coordinate system\n",
        "x_blu_x1, x_blu_x2 = 1189, 1189\n",
        "y_blu_y1, y_blu_y2 = 311, 769\n",
        "\n",
        "# Heights in image coordinate system in pixels\n",
        "y_ylw = np.sqrt((x_ylw_x1 - x_ylw_x2) ** 2 + (y_ylw_y1 - y_ylw_y2) ** 2) \n",
        "y_grn = np.sqrt((x_grn_x1 - x_grn_x2) ** 2 + (y_grn_y1 - y_grn_y2) ** 2) \n",
        "y_blu = np.sqrt((x_blu_x1 - x_blu_x2) ** 2 + (y_blu_y1 - y_blu_y2) ** 2) \n",
        "\n",
        "Z_ylw = -1 * (17 * y_grn / Y_grn) / ((y_grn/Y_grn) - (y_ylw/Y_ylw))\n",
        "f_in_mm = ((y_ylw * Z_ylw / Y_ylw ) / pixel_density_per_meter) * 1000\n",
        "Z_blu = Z_ylw + 17\n",
        "Z_grn = Z_blu + 17\n",
        "Y_blu_in_meter = (y_blu * Z_blu / f_in_mm) / pixel_density_per_meter\n",
        "y_ylw, y_grn, y_blu, Z_ylw, f_in_mm, Y_blu_in_meter"
      ],
      "metadata": {
        "id": "xQJz-VD0szTk",
        "outputId": "6efd3d5d-ecc7-48dd-f07e-519549e0ca0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(596.0302005771184,\n",
              " 309.0016181187406,\n",
              " 458.0,\n",
              " 18.301409089738772,\n",
              " 51.13247206514212,\n",
              " 0.005928771542212488)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3XiEFaVsuJZ"
      },
      "source": [
        "### Deliverables:\n",
        "\n",
        "Using the above perspective projection equations, **compute and report**:\n",
        "- Camera focal length f (in mm) : 51.13247 mm\n",
        "- 3-D height (Y) of blue pole (in m) : 5.92877 m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUGPz9iUsuJZ"
      },
      "source": [
        "## Part 2: Dolly Zoom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnqXFH-ksuJZ"
      },
      "source": [
        "![Hybrid Teaser](https://raw.githubusercontent.com/W281/fileRepository/main/Assignments/Assignment_1/dollys_zoom.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdhrtmd0suJZ"
      },
      "source": [
        "### Overview\n",
        "Wikipedia defines a dolly zoom (also known as a Hitchcock shot, Vertigo shot, Jaws effect, or Zolly shot) as an in-camera effect that appears to undermine normal visual perception. This effect has been used in many famous [movies](https://www.youtube.com/watch?v=u5JBlwlnJX0). In this part, you will create a simple 3-D scene and simulate the dolly-zoom effect, similar to that shown in Figure 2. This exercise will help you gain familiarity with transforming points between world and sensor coordinates and learn how to use Python image manipulation techniques.\n",
        "\n",
        "### Description: \n",
        "The dolly-zoom effect is accomplished by simultaneously adjusting the camera focal length and the distance between the camera and object of interest. Specifically, the effect can be achieved by moving the camera toward the object while simultaneously reducing the camera focal length such that the object remains the same size in the image and the background appears to change in volume relative to the object.\n",
        "\n",
        "In order to implement this effect, you will create a simple 3-D scene with four planes and a cube, as in Figure 2. For creating this 3-D scene, you are given the following information. \n",
        "\n",
        "Assume that your camera is looking in the positive Z-direction and has an initial camera focal-length of 300 mm. \n",
        "\n",
        "You are given the cordinates of the four corners of a fronto-parallel plane (i.e., perpendicular to camera's optical axis) with height and width equal to 400 m:  \n",
        "\n",
        "$$plane=[[0,0,0], [400,0,0], [400,400,0], [0,400,0]]$$  \n",
        "\n",
        "You are also given the cordinates of a cube of edge length 12 m and centered at the origin:\n",
        "\n",
        "$$cube=[[−6,−6,−6], [6,−6,−6], [6,6,−6], [−6,6,−6], [−6,−6,6], [6,−6,6], [6,6,6], [−6,6,6]]$$\n",
        "\n",
        "Note that both these objects are specified with respect to their own local cordinate systems with the origin at one of their vertices. In order to place the ground plane and cube in the relative locations shown in Figure 2, you need to apply the following rotation and translation to the ground plane and cube:\n",
        "\n",
        "$$ground:{Rx=90, Ry=0, Rz=0, Tx=−200, Ty=−30, Tz=100}$$\n",
        "$$cube:{Rx=30, Ry=30, Rz=0, Tx=0, Ty=0, Tz=100}$$\n",
        "\n",
        "By convention, the order of operations for 3D rotations is $$R = Rz * Ry *Rx$$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ERW3RhFsuJa"
      },
      "source": [
        "Below we've provided starter code for generating the checkerboard grid pattern, visualizing the scene, constructing the scene objects, and rendering each frame into a movie. In order to create the dolly-zoom animation shown in Figure 2 you will do the following:\n",
        "\n",
        "1. **Parse:** First you must write code to construct a transformation matrix from the given rotation, translation, and focal length parameters above. \n",
        "Fill in the `get_projective_matrix` function below. This requires parsing the input transformation parameters, constructing both the intrinsic and extrinsic matrices, and combining these together in the correct order. You must also complete the `transform_points` function by filling in the conversion step from transformed homogeneous points to non-homogenous sensor points.\n",
        "\n",
        "\n",
        "2. **Place:** We have only provided the specification for the ground plane and the cube. You must create and place the other three planes by specifying their rotations and translations. \n",
        "In the script below, fill in the correct transformation matrices for the back plane and the two side planes. All planes should be aligned exactly at their edges, should be the same size, and should not overlap or become disjointed as the camera parameters change. Make sure the entire scene is visible in the plot.\n",
        "\n",
        "\n",
        "3. **Compute:** Calculate the necessary focal lengths and transformations required to achieve the dolly zoom effect. \n",
        "The dolly zoom requires that you must move the camera and adjust the focal length together such that the cube stays roughly the same size in the projected image. (That is, conceptually, a horizontal line parallel to the image plane and running through the center of the cube would not change in length during the entire dolly zoom.) Write a function to calculate the necessary relationship between Z and f that achieves this effect.\n",
        "\n",
        "\n",
        "4. **Dolly:** After the 3-D scene is created, start moving the camera forward.  \n",
        "Move the camera forward while simultaneously adjusting the focal length. Note that moving the camera forward is the same as moving the entire scene towards the camera. Starting with an initial camera-to-cube distance of 100 meters and an initial focal length of 300 mm, and move the camera in 30 steps to a final camera-to-cube distance of 40 meters. Use your formula from step 3 to compute the necessary focal length for each frame and transform the scene accordingly. Submit the resulting GIF along with your code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO2uR3LPsuJa"
      },
      "source": [
        "### Deliverables:\n",
        "\n",
        "- Implementation of transformation matrix construction\n",
        "- Object placement in the scene\n",
        "- Implementation of dolly zoom calculation\n",
        "- Dolly zoom animation gif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWMT--nUsuJa"
      },
      "outputs": [],
      "source": [
        "# import helper functions\n",
        "# !!! Be sure to review the helper functions in utilityCode !!!\n",
        "from utilityCode import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-_bRx3ksuJa"
      },
      "source": [
        "#### Setup Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuRjkRmVsuJa"
      },
      "outputs": [],
      "source": [
        "# scene parameters as provided in the assignment\n",
        "\n",
        "f_start = 0.3 # the focal length f to start\n",
        "grid_count = 20 # the number of squares in one dimension of the plane\n",
        "Z_start = 100 # the starting distance between the camera and the cube\n",
        "Z_end = 40 # the last distance between camera and the cube\n",
        "steps = 30 # number of dolly zoom steps to take between Z_start and Z_end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pgczf4AAsuJa"
      },
      "outputs": [],
      "source": [
        "\"\"\" Given desired transformation parameters and focal length,\n",
        "    construct both the intrinsic and extrinsic matrix,\n",
        "    then return the full transformation matrix\n",
        "    Input parameter format: [Rx, Ry, Rz, Tx, Ty, Tz] \"\"\"\n",
        "\n",
        "def get_projective_matrix(in_trns, f):\n",
        "    \n",
        "    ## TODO: parse transformation params\n",
        "    # rotation params (radians)\n",
        "    # translation params\n",
        "    Rx = np.radians(30) #rotation (radians)\n",
        "    Ry = np.radians(30)\n",
        "    Rz = np.radians(0)\n",
        "    Tx = 0 #translation\n",
        "    Ty = 0\n",
        "    Tz = 100\n",
        "    \n",
        "    ## TODO: construct extrinsic matrix\n",
        "    # combine X, Y, and Z rotations\n",
        "    # append translation values\n",
        "    RMx = np.array([[1,0,0],[0,cos(Rx),-sin(Rx)],[0,sin(Rx),cos(Rx)]]) #rotation matrix\n",
        "    RMy = np.array([[cos(Ry),0,sin(Ry)],[0,1,0],[-sin(Ry),0,cos(Ry)]])\n",
        "    RMz = np.array([[cos(Rz),-sin(Rz),0],[sin(Rz),cos(Rz),0],[0,0,1]])\n",
        "    RM = RMz @ RMy @ RMx\n",
        "\n",
        "    M = np.zeros((3,4))\n",
        "    M[0,3] = Tx\n",
        "    M[1,3] = Ty\n",
        "    M[2,3] = Tz\n",
        "    M[0:3,0:3] = RM # Extrinsic Matrix\n",
        "    \n",
        "    ## TODO: construct intrinsic matrix\n",
        "    K = np.array([[f,0,0],[0,f,0],[0,0,1]]) #intrinsic matrix\n",
        "    \n",
        "    ## TODO multiply intrinsic and extrinsic matrix and return M\n",
        "    return K @ M\n",
        "\n",
        "\n",
        "\"\"\" Given an object, transform its points using its transform parameters, \n",
        "    then project it onto the sensor (f is the focal length for constructing the intrinsic matrix) \"\"\"\n",
        "\n",
        "def transform_points(in_obj, f):\n",
        "    \n",
        "    out_obj = dict(in_obj)\n",
        "    \n",
        "    XYZ = in_obj['points'].copy()\n",
        "    in_trns = in_obj['transform'].copy()\n",
        "    \n",
        "    # perspective projection\n",
        "    M = get_projective_matrix(in_trns, f)\n",
        "    \n",
        "    # projection (homogenous coordinates)\n",
        "    p = M @ np.transpose(XYZ) \n",
        "    \n",
        "    ## TODO: convert p to non-homogenous sensor coordinates x and y (rescale)\n",
        "    x = p[0,:] / p[2,:] # convert to non-homogeneous coordinates\n",
        "    y = p[1,:] / p[2,:]\n",
        "    #z = p[2,:] / p[2,:]\n",
        "    \n",
        "    # put the sensor points in the output object\n",
        "    out_obj['points'] = np.hstack(( x[:, np.newaxis], y[:, np.newaxis] ))\n",
        "    \n",
        "    return out_obj\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiNNdTXlsuJb"
      },
      "source": [
        "#### Scene Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "frGRvF_OsuJb"
      },
      "outputs": [],
      "source": [
        "\"\"\" We will use a scene dictionary with keys as object names \n",
        "    Each object will have its own keys containing object-specific points and transformation matrix\"\"\"\n",
        "\n",
        "# 3-D, unit-cube (homogenous coordinates)\n",
        "cube = np.array([ [0,0,0,1], [1,0,0,1], [1,1,0,1], [0,1,0,1],\n",
        "                  [0,0,1,1], [1,0,1,1], [1,1,1,1], [0,1,1,1]])\n",
        "\n",
        "# make length of the cube 12 meters\n",
        "cube[:, :3] = 12 * cube[:, :3]\n",
        "cube[:, :3] = cube[:, :3] - 6\n",
        "\n",
        "height = 400 # height of the walls\n",
        "plane = np.array([ [0,0,0,1], [height,0,0,1], [height,height,0,1], [0,height,0,1] ])\n",
        "\n",
        "# create an empty scene\n",
        "scene = {}\n",
        "scene['ground_plane'] = {}\n",
        "scene['right_plane'] = {}\n",
        "scene['left_plane'] = {}\n",
        "scene['back_plane'] = {}\n",
        "scene['cube'] = {}\n",
        "\n",
        "\"\"\" assign the points and transformations (Rx, Ry, Rz, Tx, Ty, Tz) \"\"\"\n",
        "\n",
        "# setup ground plane and transformation\n",
        "# put the points and transformation (Rx, Ry, Rz, Tx, Ty, Tz)\n",
        "scene['ground_plane']['points'] = np.reshape( get_plane_mesh(plane, grid_count), (-1, 4) )\n",
        "scene['ground_plane']['transform'] = np.array( [90, 0, 0, -height/2, -30, Z_start] )\n",
        "\n",
        "scene['right_plane']['points'] = np.reshape( get_plane_mesh(plane, grid_count), (-1, 4) )\n",
        "## TODO: Adjust the transformation parameters for right side plane\n",
        "scene['right_plane']['transform'] = np.array( [90, 0, 0, -height/2, -30, Z_start] )\n",
        "\n",
        "\n",
        "scene['left_plane']['points'] = np.reshape( get_plane_mesh(plane, grid_count), (-1, 4) )\n",
        "## TODO: Adjust the transformation parameters for left side plane\n",
        "scene['left_plane']['transform'] =  np.array( [90, 0, 0, -height/2, -30, Z_start] )\n",
        "\n",
        "\n",
        "scene['back_plane']['points'] = np.reshape( get_plane_mesh(plane, grid_count), (-1, 4) )\n",
        "## TODO: Adjust the transformation parameters for back plane\n",
        "scene['back_plane']['transform'] =  np.array( [90, 0, 0, -height/2, -30, Z_start] )\n",
        "\n",
        "# create and transform the cube\n",
        "scene['cube']['points'] = cube\n",
        "scene['cube']['transform'] = np.array([30, 30, 0, 0, 0, Z_start])\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBrdcdH4suJb"
      },
      "source": [
        "#### Dolly Animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQXGTv-FsuJb"
      },
      "outputs": [],
      "source": [
        "\"\"\" Perform the dolly of the entire scene, save images as a single gif \"\"\"\n",
        "\n",
        "## TODO Create the Z values for the dolly animation\n",
        "## store Z values in a numpy array object of size [steps]\n",
        "# Tzs = ???\n",
        "\n",
        "## TODO Compute the focal length to keep the cube of same size\n",
        "## store the f values for each Z step in a numpy array object of size [steps]\n",
        "## this f value should be adjusted to keep the cube the same size throughout the animation\n",
        "# fs = ???\n",
        "\n",
        "# initialize the animation file\n",
        "gif_writer = imageio.get_writer('output.gif', mode='I')\n",
        "\n",
        "# update the scene and generate images\n",
        "for i in range(steps):\n",
        "    \n",
        "    projected_scene = {}\n",
        "    for k in scene.keys(): # iterate over the objects\n",
        "        \n",
        "        # current object in the scene\n",
        "        cur_obj = copy.deepcopy(scene[k])\n",
        "        \n",
        "        ## TODO: write code to modify the Z-values of each object in the scene\n",
        "        ## hint: to understand the below assignment, look at the scene setup above and you\n",
        "        ## should see we are simply assigning a unique Z value to each object for each step in\n",
        "        ## the animation\n",
        "        # cur_obj['transform'][-1] = ???\n",
        "                \n",
        "        # project this objects on the sensor\n",
        "        projected_scene[k] = transform_points(cur_obj, fs[i])\n",
        "        \n",
        "    # visualize the scene\n",
        "    plot_img = visualize_scene(projected_scene, grid_count)\n",
        "    \n",
        "    # write the current image in the gif\n",
        "    gif_writer.append_data(plot_img)\n",
        "    \n",
        "gif_writer.close()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "994d27049b6d1b3a4dc8007fc39d9d11e995dbfa516d084782a5acb2c2c0d3bb"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}