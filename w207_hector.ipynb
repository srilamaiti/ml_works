{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/ml_works/blob/main/w207_hector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1034b34c-bc07-4790-8938-2be2b6abce88",
      "metadata": {
        "id": "1034b34c-bc07-4790-8938-2be2b6abce88"
      },
      "source": [
        "# 207 - Histopathological cancer detection\n",
        "\n",
        "Hector Rincon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bbb7521-95cc-42c2-a6f6-05cbc1050104",
      "metadata": {
        "id": "3bbb7521-95cc-42c2-a6f6-05cbc1050104"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "import random\n",
        "import gc\n",
        "import skimage.io as skio\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import Polygon\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Keras\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
        "\n",
        "# SKLearn\n",
        "from skimage.color import gray2rgb\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix\n",
        "\n",
        "# Configuration\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()"
      ]
    },
    {
      "cell_type": "raw",
      "id": "74760a36-8de5-49ab-9da2-1a5484164570",
      "metadata": {
        "id": "74760a36-8de5-49ab-9da2-1a5484164570"
      },
      "source": [
        "# if you need to download the data, run this cell. Will output to ./data/raw\n",
        "!bash ./download_data.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e40d13f1-0db8-46ae-be0f-26259642af3d",
      "metadata": {
        "id": "e40d13f1-0db8-46ae-be0f-26259642af3d",
        "outputId": "3f3a1ba7-216f-4297-a19f-c86180d3ae7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/datadrive/data\n"
          ]
        }
      ],
      "source": [
        "%cd /datadrive/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d764682-a695-4e07-8a46-2fe99ceb6a84",
      "metadata": {
        "id": "2d764682-a695-4e07-8a46-2fe99ceb6a84"
      },
      "outputs": [],
      "source": [
        "image_dir = 'raw/train'\n",
        "IMG_SIZE = 96\n",
        "BATCH_SIZE=32\n",
        "validation_dir = 'processed/validation'\n",
        "train_dir = 'processed/train'\n",
        "test_dir = 'processed/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bac02e89-655d-4eab-845f-4bda65b2f4c5",
      "metadata": {
        "id": "bac02e89-655d-4eab-845f-4bda65b2f4c5"
      },
      "source": [
        "## Create source of truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72927e6e-7e93-461f-8c8b-815cc0e47d68",
      "metadata": {
        "id": "72927e6e-7e93-461f-8c8b-815cc0e47d68"
      },
      "outputs": [],
      "source": [
        "# Read the original file\n",
        "train_label = pd.read_csv('raw/train_labels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e82cbe4e-011a-4674-8a64-5b83a6bad229",
      "metadata": {
        "id": "e82cbe4e-011a-4674-8a64-5b83a6bad229"
      },
      "outputs": [],
      "source": [
        "# Gets the count of the smaller class, and creates a new source of truth with balanced counts\n",
        "# Effectively undersamples the more populous class\n",
        "positive_examples = train_label[train_label.label == 1]\n",
        "positive_example_count = len(positive_examples)\n",
        "negative_examples = train_label[train_label.label == 0].sample(positive_example_count)\n",
        "\n",
        "# New source of truth\n",
        "df = pd.concat([positive_examples, negative_examples], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb9450f8-b8cb-4fa0-ab1f-0261617e09f2",
      "metadata": {
        "id": "bb9450f8-b8cb-4fa0-ab1f-0261617e09f2",
        "outputId": "faae5207-c1a7-48f3-e294-10223b97e102"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    89117\n",
              "0    89117\n",
              "Name: label, dtype: int64"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af0355b4-2446-41a9-8f73-50007a47242c",
      "metadata": {
        "id": "af0355b4-2446-41a9-8f73-50007a47242c"
      },
      "source": [
        "## Test/train split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7decb1c1-eeb1-4f80-8c27-e7faac079623",
      "metadata": {
        "id": "7decb1c1-eeb1-4f80-8c27-e7faac079623"
      },
      "outputs": [],
      "source": [
        "# The source of truth is train_label\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "xtrain, xtest = list(sss.split(df.id, df.label))[0]\n",
        "\n",
        "df_train = df.iloc[xtrain]\n",
        "df_test = df.iloc[xtest]\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=0) # 25% of the 80% we already got = 20% for validation\n",
        "xtrain_train, xtrain_val = list(sss2.split(df_train.id, df_train.label))[0]\n",
        "\n",
        "df_val = df_train.iloc[xtrain_val]\n",
        "df_train = df_train.iloc[xtrain_train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e8f7ded-1fc9-4a62-8e0f-1f8002d4ed16",
      "metadata": {
        "id": "9e8f7ded-1fc9-4a62-8e0f-1f8002d4ed16",
        "outputId": "741f021a-a844-4138-ba0c-83996185260e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    53470\n",
            "1    53470\n",
            "Name: label, dtype: int64\n",
            "0    17824\n",
            "1    17823\n",
            "Name: label, dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1    17824\n",
              "0    17823\n",
              "Name: label, dtype: int64"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(df_train.label.value_counts())\n",
        "print(df_test.label.value_counts())\n",
        "df_val.label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3572a7cd-ce31-4bd3-b558-751d99995754",
      "metadata": {
        "id": "3572a7cd-ce31-4bd3-b558-751d99995754"
      },
      "source": [
        "## Set up directory structure"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dccb4f4a-3537-4616-b1b0-3003f7da09a2",
      "metadata": {
        "id": "dccb4f4a-3537-4616-b1b0-3003f7da09a2"
      },
      "source": [
        "This only needs to be run once. Make sure you have enough disk space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f5c003c-b1ad-417e-90ad-434f49f25f9e",
      "metadata": {
        "id": "9f5c003c-b1ad-417e-90ad-434f49f25f9e"
      },
      "outputs": [],
      "source": [
        "def prep_directories(train_df, val_df, test_df):\n",
        "    # Essentially make 3 directories: train, validation, test\n",
        "    # each of which will have a '0' and '1' directory inside\n",
        "    root_dir = 'processed'\n",
        "    dirs = ['train', 'validation', 'test']\n",
        "\n",
        "    # Create the dirs\n",
        "    for d in dirs:\n",
        "        for c in ['0', '1']:\n",
        "            os.makedirs(f'{root_dir}/{d}/{c}', exist_ok=True)\n",
        "\n",
        "    def constrain(x, partition):\n",
        "        label = x['label']\n",
        "        imgid = x['id']\n",
        "        fullpath = f'{image_dir}/{imgid}.tif'\n",
        "        newpath = f'{root_dir}/{partition}/{label}/{imgid}.tif'\n",
        "        shutil.copyfile(fullpath, newpath)\n",
        "\n",
        "    train_df.apply(lambda x: constrain(x, 'train'), axis=1)\n",
        "    val_df.apply(lambda x: constrain(x, 'validation'), axis=1)\n",
        "    test_df.apply(lambda x: constrain(x, 'test'), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0612a86f-1302-480b-90e8-ba97653d4bbf",
      "metadata": {
        "id": "0612a86f-1302-480b-90e8-ba97653d4bbf"
      },
      "outputs": [],
      "source": [
        "prep_directories(df_train, df_val, df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a23d27cd-6aae-4e78-beed-c4f220f2b907",
      "metadata": {
        "id": "a23d27cd-6aae-4e78-beed-c4f220f2b907"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.config.run_functions_eagerly(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09f0bf05-843a-40db-908c-3ed2f8eefbbe",
      "metadata": {
        "id": "09f0bf05-843a-40db-908c-3ed2f8eefbbe"
      },
      "source": [
        "# Test image data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2038776a-b138-41e9-9101-67c3fb225795",
      "metadata": {
        "id": "2038776a-b138-41e9-9101-67c3fb225795"
      },
      "outputs": [],
      "source": [
        "def rotate_img(image):\n",
        "    return np.rot90(image, np.random.choice([-1, 0, 1, 2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e86ccf5-d4ea-4aa3-9101-5df41ef718ec",
      "metadata": {
        "id": "7e86ccf5-d4ea-4aa3-9101-5df41ef718ec",
        "outputId": "a1bd7e6a-ddd4-4176-f0d4-e56b77a1c1ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Generators\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    preprocessing_function=rotate_img,\n",
        "    brightness_range=[0.4, 1.2]\n",
        ")\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc9277c9-0935-4d1f-9ed4-246a6e787454",
      "metadata": {
        "id": "cc9277c9-0935-4d1f-9ed4-246a6e787454"
      },
      "source": [
        "# CNN model v2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc2b70d7-258e-44b1-9d23-f14f90e5728a",
      "metadata": {
        "id": "bc2b70d7-258e-44b1-9d23-f14f90e5728a"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e971c2e2-e5a8-4542-a2e1-940622a5764f",
      "metadata": {
        "id": "e971c2e2-e5a8-4542-a2e1-940622a5764f",
        "outputId": "cc0b39a0-3819-4dc8-dd15-a1701e86ba97"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-13 22:20:40.873185: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-11-13 22:20:40.873220: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-11-13 22:20:40.873239: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (w207): /proc/driver/nvidia/version does not exist\n",
            "2022-11-13 22:20:40.873468: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "input_shape = (IMG_SIZE, IMG_SIZE, 1)\n",
        "model_cnn_2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3,3), padding='same', activation=tf.nn.relu, input_shape=input_shape),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
        "    tf.keras.layers.Dropout(.1),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
        "    tf.keras.layers.Dropout(.1),\n",
        "\n",
        "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
        "    tf.keras.layers.Dropout(.1),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    \n",
        "    tf.keras.layers.Dense(1, activation= None)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad7b0784-dff0-4898-9116-42a9156666ca",
      "metadata": {
        "id": "ad7b0784-dff0-4898-9116-42a9156666ca"
      },
      "outputs": [],
      "source": [
        "model_cnn_2.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),  metrics = ['accuracy']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4259094b-3915-4909-b803-d41549f1fb2a",
      "metadata": {
        "id": "4259094b-3915-4909-b803-d41549f1fb2a",
        "outputId": "c2f9446a-492d-48b7-b77b-87822fb3c652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 106940 images belonging to 2 classes.\n",
            "Found 35647 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen_flow = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    class_mode='binary',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    color_mode='grayscale'\n",
        ")\n",
        "val_datagen_flow = val_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    class_mode='binary',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    color_mode='grayscale'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d27ad7-1ac4-46e3-9f4d-e40f70766377",
      "metadata": {
        "id": "16d27ad7-1ac4-46e3-9f4d-e40f70766377",
        "outputId": "92862543-e85e-4cd4-aab8-abaeb2c32159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3342/3342 [==============================] - 516s 154ms/step - loss: 0.5096 - accuracy: 0.7339 - val_loss: 0.4814 - val_accuracy: 0.7666\n",
            "Epoch 2/10\n",
            "3342/3342 [==============================] - 512s 153ms/step - loss: 0.4564 - accuracy: 0.7790 - val_loss: 0.4349 - val_accuracy: 0.7867\n",
            "Epoch 3/10\n",
            "3342/3342 [==============================] - 509s 152ms/step - loss: 0.4294 - accuracy: 0.7948 - val_loss: 0.4614 - val_accuracy: 0.7667\n",
            "Epoch 4/10\n",
            "3342/3342 [==============================] - 510s 152ms/step - loss: 0.4140 - accuracy: 0.8053 - val_loss: 0.4523 - val_accuracy: 0.7961\n",
            "Epoch 5/10\n",
            "3342/3342 [==============================] - 509s 152ms/step - loss: 0.3975 - accuracy: 0.8142 - val_loss: 0.4002 - val_accuracy: 0.7947\n",
            "Epoch 6/10\n",
            "3342/3342 [==============================] - 509s 152ms/step - loss: 0.3844 - accuracy: 0.8219 - val_loss: 0.3714 - val_accuracy: 0.8168\n",
            "Epoch 7/10\n",
            "3342/3342 [==============================] - 509s 152ms/step - loss: 0.3773 - accuracy: 0.8248 - val_loss: 0.3936 - val_accuracy: 0.8016\n",
            "Epoch 8/10\n",
            "3342/3342 [==============================] - 509s 152ms/step - loss: 0.3679 - accuracy: 0.8317 - val_loss: 0.3951 - val_accuracy: 0.8327\n",
            "Epoch 9/10\n",
            "3342/3342 [==============================] - 510s 153ms/step - loss: 0.3639 - accuracy: 0.8334 - val_loss: 0.3916 - val_accuracy: 0.8179\n",
            "Epoch 10/10\n",
            "3342/3342 [==============================] - 510s 153ms/step - loss: 0.3587 - accuracy: 0.8360 - val_loss: 0.3778 - val_accuracy: 0.8089\n"
          ]
        }
      ],
      "source": [
        "history_model_cnn_2 = model_cnn_2.fit(train_datagen_flow,\n",
        "                                      epochs=10,\n",
        "                                      steps_per_epoch=len(train_datagen_flow),\n",
        "                                      validation_data=val_datagen_flow,\n",
        "                                      validation_steps=len(val_datagen_flow),\n",
        "                                      callbacks=[CSVLogger('training_logs.csv', append=False, separator=';')]\n",
        "                                     )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c669680-ed2f-458d-9712-930f124b71cd",
      "metadata": {
        "id": "2c669680-ed2f-458d-9712-930f124b71cd"
      },
      "source": [
        "# Densenet201"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1658ee15-7faf-41e7-b83a-3b62154d68e9",
      "metadata": {
        "id": "1658ee15-7faf-41e7-b83a-3b62154d68e9",
        "outputId": "f1b82f4c-8977-4556-ebd8-1056cc286e93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 106940 images belonging to 2 classes.\n",
            "Found 35647 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen_flow_color = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    class_mode='binary',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    color_mode='rgb'\n",
        ")\n",
        "val_datagen_flow_color = val_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    class_mode='binary',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    color_mode='rgb'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c257af8e-e193-4266-82bd-eb59c05da499",
      "metadata": {
        "id": "c257af8e-e193-4266-82bd-eb59c05da499",
        "outputId": "329628f2-0dbf-4e89-8ad0-7e7c1764673e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "74836368/74836368 [==============================] - 3s 0us/step\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential_1 (Sequential)   (None, 3, 3, 1920)        18321984  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 1920)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1920)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 1921      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,323,905\n",
            "Trainable params: 1,921\n",
            "Non-trainable params: 18,321,984\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "base_densenet_model =  Sequential([\n",
        "    tf.keras.applications.densenet.DenseNet201(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        "    )\n",
        "])\n",
        "base_densenet_model.trainable = False\n",
        "\n",
        "densenet_model = Sequential([\n",
        "    base_densenet_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "densenet_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd0d2088-f4f7-46fe-bf71-e52c6e5d77ef",
      "metadata": {
        "id": "dd0d2088-f4f7-46fe-bf71-e52c6e5d77ef"
      },
      "outputs": [],
      "source": [
        "densenet_model.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),  metrics = ['accuracy']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f64538fc-7561-4dae-8b29-04df51e42836",
      "metadata": {
        "id": "f64538fc-7561-4dae-8b29-04df51e42836"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/54527760/using-tensorflow-how-do-i-find-the-time-taken-for-an-epoch-during-fitting\n",
        "import time\n",
        "class timecallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        # use this value as reference to calculate cummulative time taken\n",
        "        self.timetaken = time.process_time()\n",
        "    def on_epoch_end(self,epoch,logs = {}):\n",
        "        self.times.append((epoch,time.process_time() - self.timetaken))\n",
        "    def on_train_end(self,logs = {}):\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Total time taken until an epoch in seconds')\n",
        "        plt.plot(*zip(*self.times))\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec750f21-ae86-4eda-987c-7581a8a710f1",
      "metadata": {
        "id": "ec750f21-ae86-4eda-987c-7581a8a710f1",
        "outputId": "8ef1faa7-0f23-4df6-858d-14780e42c4d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3342/3342 [==============================] - 1857s 553ms/step - loss: 0.4278 - accuracy: 0.7986 - val_loss: 0.3783 - val_accuracy: 0.8202\n",
            "Epoch 2/10\n",
            "3342/3342 [==============================] - 1845s 552ms/step - loss: 0.4092 - accuracy: 0.8120 - val_loss: 0.3760 - val_accuracy: 0.8165\n",
            "Epoch 3/10\n",
            "1179/3342 [=========>....................] - ETA: 15:08 - loss: 0.4047 - accuracy: 0.8160"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The Jupyter server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--ServerApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "ServerApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3342/3342 [==============================] - 1842s 551ms/step - loss: 0.4069 - accuracy: 0.8131 - val_loss: 0.3721 - val_accuracy: 0.8232\n",
            "Epoch 5/10\n",
            "1622/3342 [=============>................] - ETA: 12:01 - loss: 0.4050 - accuracy: 0.8132"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The Jupyter server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--ServerApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "ServerApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3342/3342 [==============================] - 1840s 551ms/step - loss: 0.4036 - accuracy: 0.8147 - val_loss: 0.3545 - val_accuracy: 0.8319\n",
            "Epoch 9/10\n",
            " 543/3342 [===>..........................] - ETA: 19:33 - loss: 0.4040 - accuracy: 0.8155"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The Jupyter server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--ServerApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "ServerApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "time_callback = timecallback()\n",
        "history_densetmodel = densenet_model.fit(train_datagen_flow_color,\n",
        "                                      epochs=10,\n",
        "                                      steps_per_epoch=len(train_datagen_flow_color),\n",
        "                                      validation_data=val_datagen_flow_color,\n",
        "                                      validation_steps=len(val_datagen_flow_color),\n",
        "                                      callbacks=[time_callback, CSVLogger('training_logs_densenet.csv', append=False, separator=',')]\n",
        "                                     )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d97442f0-2adb-4249-b315-d17c7032033c",
      "metadata": {
        "id": "d97442f0-2adb-4249-b315-d17c7032033c"
      },
      "source": [
        "# RESNET50"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7466c40-11bb-4bd5-aa03-696c3fef6132",
      "metadata": {
        "id": "e7466c40-11bb-4bd5-aa03-696c3fef6132"
      },
      "source": [
        "## Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c27b9a-5d5e-4e5b-a9f7-2486730f54d1",
      "metadata": {
        "id": "31c27b9a-5d5e-4e5b-a9f7-2486730f54d1"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "rnet50 = tf.keras.applications.resnet50.ResNet50(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_tensor=None,\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        ")\n",
        "\n",
        "inputs = tf.keras.Input(input_shape)\n",
        "m2 = GlobalAveragePooling2D()(rnet50(inputs))\n",
        "m2 = tf.keras.layers.Dropout(.1)(m2)\n",
        "m2 = tf.keras.layers.Flatten()(m2)\n",
        "m2 = tf.keras.layers.Dense(1, activation= None)(m2)\n",
        "\n",
        "resnetmodel = tf.keras.Model(inputs=inputs, outputs=m2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a35de41-9134-4c54-ba90-d0a00aa38a6a",
      "metadata": {
        "id": "6a35de41-9134-4c54-ba90-d0a00aa38a6a"
      },
      "source": [
        "## Compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59eef56a-6500-495f-937b-e31f6045d3f9",
      "metadata": {
        "id": "59eef56a-6500-495f-937b-e31f6045d3f9"
      },
      "outputs": [],
      "source": [
        "resnetmodel.compile(optimizer = 'adam',\n",
        "              loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),  \n",
        "              metrics = ['accuracy']) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e771929-6a57-4e69-98cf-d2f7a830371c",
      "metadata": {
        "id": "2e771929-6a57-4e69-98cf-d2f7a830371c"
      },
      "source": [
        "## Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "541182fe-02cd-4b30-9aca-a5a92e1963ef",
      "metadata": {
        "id": "541182fe-02cd-4b30-9aca-a5a92e1963ef"
      },
      "outputs": [],
      "source": [
        "history_resnetmodel = resnetmodel.fit(np.repeat(X_train, 3, -1), y_train,\n",
        "                    epochs = 10, \n",
        "                    validation_data = (np.repeat(X_val, 3, -1), y_val)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8180cfde-47c0-4deb-a7b9-946a1e755522",
      "metadata": {
        "id": "8180cfde-47c0-4deb-a7b9-946a1e755522"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "476a61f6-8a99-4379-9c63-e3982527a926",
      "metadata": {
        "id": "476a61f6-8a99-4379-9c63-e3982527a926"
      },
      "outputs": [],
      "source": [
        "resnetmodel.save('/home/hector/resnest_model.tf', overwrite=True, include_optimizer=True, save_format='tf')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}