{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs9Je-MNINnR"
      },
      "source": [
        "## First What is BERT?\n",
        "\n",
        "BERT stands for Bidirectional Encoder Representations from Transformers. The name itself gives us several clues to what BERT is all about.\n",
        "\n",
        "BERT architecture consists of several Transformer encoders stacked together. Each Transformer encoder encapsulates two sub-layers: a self-attention layer and a feed-forward layer.\n",
        "\n",
        "### There are two different BERT models:\n",
        "\n",
        "- BERT base, which is a BERT model consists of 12 layers of Transformer encoder, 12 attention heads, 768 hidden size, and 110M parameters.\n",
        "\n",
        "- BERT large, which is a BERT model consists of 24 layers of Transformer encoder,16 attention heads, 1024 hidden size, and 340 parameters.\n",
        "\n",
        "\n",
        "\n",
        "BERT Input and Output\n",
        "BERT model expects a sequence of tokens (words) as an input. In each sequence of tokens, there are two special tokens that BERT would expect as an input:\n",
        "\n",
        "- [CLS]: This is the first token of every sequence, which stands for classification token.\n",
        "- [SEP]: This is the token that makes BERT know which token belongs to which sequence. This special token is mainly important for a next sentence prediction task or question-answering task. If we only have one sequence, then this token will be appended to the end of the sequence.\n",
        "\n",
        "\n",
        "It is also important to note that the maximum size of tokens that can be fed into BERT model is 512. If the tokens in a sequence are less than 512, we can use padding to fill the unused token slots with [PAD] token. If the tokens in a sequence are longer than 512, then we need to do a truncation.\n",
        "\n",
        "And that’s all that BERT expects as input.\n",
        "\n",
        "BERT model then will output an embedding vector of size 768 in each of the tokens. We can use these vectors as an input for different kinds of NLP applications, whether it is text classification, next sentence prediction, Named-Entity-Recognition (NER), or question-answering.\n",
        "\n",
        "\n",
        "------------\n",
        "\n",
        "**For a text classification task**, we focus our attention on the embedding vector output from the special [CLS] token. This means that we’re going to use the embedding vector of size 768 from [CLS] token as an input for our classifier, which then will output a vector of size the number of classes in our classification task.\n",
        "\n",
        "-----------------------\n",
        "\n",
        "![Imgur](https://imgur.com/NpeB9vb.png)\n",
        "\n",
        "-------------------------\n",
        "\n",
        "The core part of BERT is the stacked bidirectional encoders from the transformer model, but during pre-training, a masked language modeling and next sentence prediction head are added onto BERT.\n",
        "\n",
        "When I say “head”, I mean that a few extra layers are added onto BERT that can be used to generate a specific output. The raw output of BERT is the output from the stacked Bi-directional encoders.\n",
        "\n",
        "### Tokenization for Bert Models\n",
        "\n",
        "Tokenization plays an essential role in NLP as it helps convert the text to numbers which deep learning models can use for processing.\n",
        "\n",
        "No deep learning models can work directly with the text. You need to convert it into numbers or the format which the model can understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox-QTZa31D6u"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnfvaKxj1cs9",
        "outputId": "bdc1fe09-5235-4aa0-d173-f4d28c04abc5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101, 16798,  2475,  2097,  2022,  1037,  2307,  2095,  2005,  2035,\n",
              "          1997,  2149,   102,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]])}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = '2022 will be a great year for all of us'\n",
        "encoding = tokenizer.encode_plus(text, add_special_tokens = True, truncation = True, padding = \"max_length\", return_attention_mask = True, return_tensors = \"pt\")\n",
        "encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lppcI2PfINnY"
      },
      "source": [
        "**`padding = \"max_length\"`** => The padding argument controls padding. padded by zeros to make all the text to the length of `max_length`. So, it will pad to a length specified by the `max_length` argument or the maximum length accepted by the model if no max_length is provided. Padding will still be applied if you only provide a single sequence.\n",
        "\n",
        "**truncation = True** => True or 'longest_first': truncate to a maximum length specified by the max_length argument or the maximum length accepted by the model if no max_length is provided (max_length=None). This will truncate token by token, removing a token from the longest sequence in the pair until the proper length is reached.\n",
        "\n",
        "As BERT can only accept as input only 512 tokens at a time, so we must specify the truncation parameter to True. \n",
        "\n",
        "The **`add_special_tokens`** parameter is just for BERT to add tokens like the start, end, [SEP], and [CLS] tokens. `Return_tensors = “pt”` is just for the tokenizer to return PyTorch tensors. If you don’t want this to happen(maybe you want it to return a list), then you can remove the parameter and it will return lists.\n",
        "\n",
        "**tokenizer.encode_plus()** specifically returns a dictionary of values instead of just a list of values. Because tokenizer.encode_plus() can return many different types of information, like the attention_masks and token type ids, everything is returned in a dictionary format, and if you want to retrieve the specific parts of the encoding, you can do it like below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqBlnJiw1cpR"
      },
      "outputs": [],
      "source": [
        "input = encoding[\"input_ids\"][0]\n",
        "attention_mask = encoding[\"attention_mask\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZGRYZtzINnZ"
      },
      "source": [
        "Additionally, because the tokenizer returns a dictionary of different values, instead of finding those values as shown above and individually passing these into the model, we can just pass in the entire encoding like this\n",
        "\n",
        "```\n",
        "output = model(**encoding)\n",
        "```\n",
        "\n",
        "\n",
        "One more very important thing about the tokenizer to know is that you can specify to retrieve specific tokens if desired. For example, if you are doing masked language modeling and you want to insert a mask at a location for your model to decode, then you can simply retrieve the mask token like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DfO6a3SJINnZ",
        "outputId": "e5195862-c639-4e03-a754-fee29b56af56"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[MASK]'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.mask_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnd_beq0INna"
      },
      "source": [
        "## Masked Language Modeling\n",
        "\n",
        "Masked Language modelling is a way to perform word prediction that was originally hidden intentionally in a sentence.\n",
        "\n",
        "In simple terms, it is the task of filling in the blanks.\n",
        "\n",
        "Masked language modelling can be considered similar to autoencoding modelling which works based on constructing outcomes from unarranged or corrupted input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294,
          "referenced_widgets": [
            "daad56b36f624745a95609ee26a537c1",
            "961e2c7863c24f7aacf5a82d917f2d5a",
            "538ac7b44e4644cf9702694e2cfd51c9",
            "a8e1ba0497a2461782ae307d957b7035",
            "0015217dbc9547979e244b044d45e5f4",
            "860a7d8016164e9ba0156fa98d6cf269",
            "95408d7e0b8044b8b6bfc3a011992c44",
            "710533b9a5a1494d95aa7b3e99ca307a",
            "ac888a1f4b764839a5e75ec3876d56f5",
            "67861cf7f6dc44e59e9850e897905a4b",
            "307fdf82a7a34cb891aa5e6221ebc126"
          ]
        },
        "id": "FnNpzW3m1cjV",
        "outputId": "dbf94d38-5231-4643-e0fe-eca4d1b17e17"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "daad56b36f624745a95609ee26a537c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Opera House in Australia is in , sydney city\n",
            "The Opera House in Australia is in , melbourne city\n",
            "The Opera House in Australia is in , brisbane city\n",
            "The Opera House in Australia is in , adelaide city\n",
            "The Opera House in Australia is in , the city\n",
            "The Opera House in Australia is in , canberra city\n",
            "The Opera House in Australia is in , auckland city\n",
            "The Opera House in Australia is in , hobart city\n",
            "The Opera House in Australia is in , griffith city\n",
            "The Opera House in Australia is in , hume city\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased', return_dict = True)\n",
        "\n",
        "''' Masked Language Modeling works by inserting a mask token at the desired position where you want to predict the best candidate word that would go in that position.\n",
        "\n",
        "You can simply insert the mask token by concatenating it at the desired position\n",
        "\n",
        "The Bert Model for Masked Language Modeling predicts the best word/token in its vocabulary that would replace that word. \n",
        "\n",
        "The logits are the output of the BERT Model before a softmax activation function is applied to the output of BERT. \n",
        "i.e. logits are the Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
        "\n",
        "And in order to get the logits, we have to specify \"return_dict = True\" in the parameters when initializing the model, otherwise, the above code will result in a compilation error. \n",
        "\n",
        "\"return_dict\" - If set to True, the model will return a ModelOutput class instead of a plain tuple.\n",
        "\n",
        "On \"return_dict = True\" see my note - HuggingFace/input_dict_true_its_purpose.ipynb\n",
        "\n",
        "'''\n",
        "\n",
        "text = \"The Opera House in Australia is in , \" + tokenizer.mask_token + \" city\"\n",
        "\n",
        "input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
        "\n",
        "\n",
        "''' In order to get the tensor of softmax values of all the words in BERT’s vocabulary for replacing the mask token, we need to specify the masked token index.\n",
        "\n",
        "And these we can get using torch.where(). And in this particular example I am retrieving the top 10 candidate replacement words for the mask token. '''\n",
        "mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
        "\n",
        "''' mask_token (str or tokenizers.AddedToken, optional) — A special token representing a masked token (used by masked-language modeling pretraining objectives, like BERT). Will be associated to self.mask_token and self.mask_token_id. '''\n",
        "\n",
        "output = model(**input)\n",
        "\n",
        "logits = output.logits\n",
        "\n",
        "''' After we pass the input encoding into the BERT Model, we can get the logits simply by specifying output.logits, which returns a tensor, and after this we can finally apply a softmax activation function to the logits. '''\n",
        "\n",
        "softmax = F.softmax(logits, dim = -1)\n",
        "''' By applying a softmax onto the output of BERT, we get probabilistic distributions for each of the words in BERT’s vocabulary. Word’s with a higher probability value will be better candidate replacement words for the mask token.  '''\n",
        "\n",
        "mask_word = softmax[0, mask_index, :]\n",
        "''' In order to get the tensor of softmax values of all the words in BERT’s vocabulary for replacing the mask token, we can specify the masked token index, which we already got using torch.where(). \n",
        "\n",
        "Further, Because in this particular example I am retrieving the top 10 candidate replacement words for the mask token. '''\n",
        "\n",
        "top_10 = torch.topk(mask_word, 10, dim = 1)[1][0]\n",
        "''' I used the torch.topk() function, which allows you to retrieve the top k values in a given tensor, and it returns a tensor containing those top k values. '''\n",
        "\n",
        "'''  After this, the process becomes relatively simple, as all we have to do is iterate through the tensor, and replace the mask token in the sentence with the candidate token. '''\n",
        "for token in top_10:\n",
        "   word = tokenizer.decode([token])\n",
        "   new_sentence = text.replace(tokenizer.mask_token, word)\n",
        "   print(new_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1Aq2bb2INnc"
      },
      "source": [
        "If you want to only get the top candidate word, you can do this:\n",
        "\n",
        "```py\n",
        "softmax = F.softmax(logits, dim = -1)\n",
        "\n",
        "mask_word = softmax[0, mask_index, :]\n",
        "\n",
        "top_word = torch.argmax(mask_word, dim=1)\n",
        "\n",
        "print(tokenizer.decode(top_word))\n",
        "```\n",
        "\n",
        "Instead of using torch.topk() for retrieving the top 10 values, we just use torch.argmax(), which returns the index of the maximum value in the tensor. The rest of the code is pretty much the same thing as the original code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4C51g_XINnd"
      },
      "source": [
        "---\n",
        "\n",
        "## Next Sentence Prediction\n",
        "\n",
        "Next Sentence Prediction is the task of predicting whether one sentence follows another sentence. Here is my code for this:\n",
        "\n",
        "\n",
        "[BertForNextSentencePrediction](https://huggingface.co/transformers/v4.9.2/model_doc/bert.html#bertfornextsentenceprediction)\n",
        "\n",
        "It returns logits (torch.FloatTensor of shape (batch_size, 2)) – Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dru3GChd1cdI",
        "outputId": "7e1608d8-2178-4854-9147-bed300ba1f59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[9.9998e-01, 1.5085e-05]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "\n",
        "prompt = \"I came back from Office in the evening\"\n",
        "\n",
        "next_sentence = \"I opened my Beer after Office\"\n",
        "\n",
        "encoding = tokenizer.encode_plus(prompt, next_sentence, return_tensors='pt')\n",
        "outputs = model(**encoding)[0]\n",
        "softmax = F.softmax(outputs, dim = 1)\n",
        "print(softmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_rEIxlTINne"
      },
      "source": [
        "Next Sentence prediction is the task of predicting how good a sentence is as a next sentence for a given sentence. \n",
        "\n",
        "In this case, prompt variable is the given sentence and we are trying to predict whether next_sentence is the next sentence. \n",
        "\n",
        "\n",
        "To do this, the BERT tokenizer automatically inserts a [SEP] token in between the sentences, which represents the separation between the two sentences, and the specific Bert \n",
        "\n",
        "For Next Sentence Prediction model predicts two values of whether the sentence is the next sentence. \n",
        "\n",
        "Bert returns two values in a tensor: the first value represents whether the second sentence is a continuation of the first, and the second value represents whether the second sentence is a random sequence or not a good continuation of the first. \n",
        "\n",
        "### Unlike Masked Language Modeling, we don’t retrieve any logits because we are not trying to compute a softmax on the vocabulary of BERT; \n",
        "\n",
        "### We are simply trying to compute a softmax on the two values that BERT for next sentence prediction returns so that we can see which value has the highest probability. \n",
        "\n",
        "### And this will represent whether the second sentence is a good next sentence for the first. \n",
        "\n",
        "\n",
        "Once we get the softmax values, we can simply look at the tensor by printing it out. Here are the values that I got:\n",
        "\n",
        "`tensor([[0.9953, 0.0047]], grad_fn=<SoftmaxBackward0>)`\n",
        "\n",
        "Because the first value is considerably higher than the second index, BERT believes that the second sentence follows the first sentence, which is the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZVdtgwm35R2"
      },
      "source": [
        "---\n",
        "\n",
        "## Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHDS1xjWnLTk"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
        "\n",
        "example_text = \"GPT-3 came in 2020\"\n",
        "\n",
        "example_question = \"When did GPT-3 come\"\n",
        "\n",
        "# We can use our tokenizer to automatically generate 2 sentence by passing the\n",
        "# two sequences to tokenizer as two arguments\n",
        "tokenized_inputs = tokenizer(example_question, example_text, return_tensors=\"pt\")\n",
        "tokenized_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_g_p-konLTk"
      },
      "source": [
        "\n",
        "Bert QA appends question before context.\n",
        "\n",
        "Tokenizer returns 3 tensors for us.\n",
        "\n",
        "#### “inputs_ids” are tokenized ids of text.\n",
        "\n",
        "----------------------\n",
        "\n",
        "### \"'token_type_ids' => To understand them first note, Some models’ purpose is to do classification on pairs of sentences or question answering.\n",
        "\n",
        "https://huggingface.co/docs/transformers/v4.20.1/en/glossary#token-type-ids\n",
        "\n",
        "These require two different sequences to be joined in a single “input_ids” entry, which usually is performed with the help of special tokens, such as the classifier ([CLS]) and separator ([SEP]) tokens. For example, the BERT model builds its two sequence input as such:\n",
        "\n",
        "\n",
        "### [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]\n",
        "\n",
        "We used our tokenizer to automatically generate such a sentence by passing the two sequences to tokenizer as two arguments\n",
        "\n",
        "BERT has token type IDs (also called segment IDs). They are represented as a binary mask identifying the two types of sequence in the model.\n",
        "\n",
        "Here those 2 types of sequences are Questions and the Context.\n",
        "\n",
        "Token type 0 is for question part and 1 context.\n",
        "\n",
        "\n",
        "**The model will tell you at what start and end position of the input_ids the answer to the question will be located.**\n",
        "\n",
        "----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr2kibJtnLTk"
      },
      "outputs": [],
      "source": [
        "text = \"The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula.   The Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail.   In March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online.   The Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items.   Scholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican.   The Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.\"\n",
        "\n",
        "question = \"When was the Vat formally opened?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okTIrHVt2BuE"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
        "\n",
        "tokenized_inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**tokenized_inputs)\n",
        "\n",
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()\n",
        "\n",
        "''' start_logits (torch.FloatTensor of shape (batch_size, sequence_length)) — Span-start scores (before SoftMax).\n",
        "\n",
        "end_logits (torch.FloatTensor of shape (batch_size, sequence_length)) — Span-end scores (before SoftMax). '''\n",
        "\n",
        "predict_answer_tokens = tokenized_inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "tokenizer.decode(predict_answer_tokens)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "BERT from Hugging Face - Few Baseline Usages.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0015217dbc9547979e244b044d45e5f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "307fdf82a7a34cb891aa5e6221ebc126": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "538ac7b44e4644cf9702694e2cfd51c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_710533b9a5a1494d95aa7b3e99ca307a",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac888a1f4b764839a5e75ec3876d56f5",
            "value": 440473133
          }
        },
        "67861cf7f6dc44e59e9850e897905a4b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "710533b9a5a1494d95aa7b3e99ca307a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "860a7d8016164e9ba0156fa98d6cf269": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95408d7e0b8044b8b6bfc3a011992c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "961e2c7863c24f7aacf5a82d917f2d5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_860a7d8016164e9ba0156fa98d6cf269",
            "placeholder": "​",
            "style": "IPY_MODEL_95408d7e0b8044b8b6bfc3a011992c44",
            "value": "Downloading: 100%"
          }
        },
        "a8e1ba0497a2461782ae307d957b7035": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67861cf7f6dc44e59e9850e897905a4b",
            "placeholder": "​",
            "style": "IPY_MODEL_307fdf82a7a34cb891aa5e6221ebc126",
            "value": " 420M/420M [00:16&lt;00:00, 39.3MB/s]"
          }
        },
        "ac888a1f4b764839a5e75ec3876d56f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "daad56b36f624745a95609ee26a537c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_961e2c7863c24f7aacf5a82d917f2d5a",
              "IPY_MODEL_538ac7b44e4644cf9702694e2cfd51c9",
              "IPY_MODEL_a8e1ba0497a2461782ae307d957b7035"
            ],
            "layout": "IPY_MODEL_0015217dbc9547979e244b044d45e5f4"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}